{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16f4327-eccc-454a-8cdc-ef281bad9239",
   "metadata": {},
   "source": [
    "# Dedupe Package\n",
    "The dupe package helps you to identify duplicated values in a data set that should be unique.  This package was created to assist data visualization, science, and engineering hobbiests and professionals.  Improper joins, poorly created data sets, or queries can cause problems with data in terms of level of detail leading to inaccuracies.  Analysis of duplicated values takes time and this package aims to return time back to more important tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28992a6e-02b2-4f5e-be55-05736f1119bb",
   "metadata": {},
   "source": [
    "## Import required packages\n",
    "This package relies on polars instead of pandas for the lazy option and the better speed.  There's no way to know how much data will be thrown at this package so optimizing for speed in the abstract is important for performance once in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e45a0e1-2d0c-4756-887f-a35f2921f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dc850-fe2b-4b59-9cfd-0127e917cb2b",
   "metadata": {},
   "source": [
    "## Dupe Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb818c1-64d5-48e6-94bd-ec22579558af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create class object\n",
    "class Dupe:\n",
    "    def __init__(self, data, key = None, suggest_key = False):\n",
    "        self.data = data\n",
    "        self.key = key\n",
    "        self.suggest_key = suggest_key\n",
    "        self.dupe_data = None\n",
    "\n",
    "    \n",
    "    ## Function to set the Dupe class item key, or column that should be unique\n",
    "    def set_key(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    \n",
    "    ## Function to get information about the data and key in the Dupe class object\n",
    "    def get_info(self):\n",
    "        message = ''\n",
    "        if self.key == None:\n",
    "            message += 'No key is currently assigned\\n'\n",
    "        else:\n",
    "            message += f'Key value: {self.key}\\n'\n",
    "\n",
    "        s = self.data.shape\n",
    "        message += f'Your data includes {s[0]} rows and {s[1]} columns'\n",
    "        print(message)\n",
    "\n",
    "    \n",
    "    ## Function to show the head of the dataframe in the Dupe class object, defaults to five rows\n",
    "    def show_data(self, rows = 5):\n",
    "        return self.data.head(rows)\n",
    "\n",
    "    \n",
    "    ## Function that suggests best candidates for unique ids in dataframe\n",
    "    def get_key_suggestion(self, n_suggestions = 3):\n",
    "        ## Get unique values for each column        \n",
    "        cols = {'column': [], 'unique': []}\n",
    "        uni = 0\n",
    "        \n",
    "        for c in self.data.columns:\n",
    "            cols['column'].append(c)\n",
    "            cols['unique'].append(self.data[c].unique().shape[0])\n",
    "        \n",
    "        ## Check if any columns are completely unique\n",
    "        suggested_keys = []\n",
    "        df_rows = self.data.shape[0]\n",
    "        sug_key = pl.DataFrame(cols)\n",
    "        \n",
    "        ## Return a list of completely unique columns\n",
    "        unique_cols = sug_key.filter(pl.col('unique') == df_rows).shape[0]\n",
    "        if unique_cols > 0:\n",
    "            suggested_keys = sug_key.filter(pl.col('unique') == df_rows)['column'].to_list()\n",
    "            if len(suggested_keys) == 1:\n",
    "                message = 'Your data includes one unique id\\n'\n",
    "            else:\n",
    "                message = 'Your data includes unique ids\\n'\n",
    "        \n",
    "        ## Otherwise, return the top N options based on highest unique values\n",
    "        else:\n",
    "            suggested_keys = sug_key.sort('unique', descending = True).head(n_suggestions)['column'].to_list()\n",
    "            message = f'The top {n_suggestions} suggestions for a unique id include:\\n'\n",
    "\n",
    "        key_messages = []\n",
    "        key_message = ''\n",
    "        pct_uni = 0.0\n",
    "        bullet = 1\n",
    "        \n",
    "        for key in suggested_keys:\n",
    "            unique_count = sug_key.filter(pl.col('column') == key).select('unique').item()\n",
    "            pct_uni = f'{unique_count / df_rows * 100:.2f}%'\n",
    "            \n",
    "            key_message = f\"\\t{bullet}. {key}: {pct_uni} unique ({unique_count} of {df_rows} unique)\"\n",
    "            key_messages.append(key_message)\n",
    "            bullet += 1\n",
    "\n",
    "        key_messages = '\\n'.join(key_messages)\n",
    "        print(message + key_messages)\n",
    "\n",
    "    ## Function that identifies the columns responsible for duplicating a specified column\n",
    "    def find_dupe_cols(self, ignore_cols = []):\n",
    "        print('Setting everything up')\n",
    "\n",
    "        dfd = self.data\n",
    "        key = self.key\n",
    "        unique_keys = self.data[key].unique().to_list()\n",
    "        uk_len = len(unique_keys)\n",
    "        uk_rows, col_rows = 0, 0\n",
    "        uk_df = pl.DataFrame()\n",
    "        key_header = f'{key}_value'\n",
    "        dupe_dict = {key_header: [], 'dupe_col': [], 'dupe_count': [], 'dupe_vals': []}\n",
    "\n",
    "        print(f'Starting to process {uk_len} rows of data')\n",
    "        \n",
    "        for uk in unique_keys:\n",
    "            uk_df = self.data.select(key).filter(pl.col(key) == uk)\n",
    "            uk_rows = uk_df.shape[0]\n",
    "\n",
    "            if uk_rows > 1:\n",
    "                for col in [col for col in dfd.columns if col != key]:\n",
    "                    uk_df = dfd.filter(pl.col(key) == uk).select([key, col]).unique()\n",
    "                    col_rows = uk_df.shape[0]\n",
    "                    if col_rows > 1:\n",
    "                        diff_vals = uk_df[col].to_list()\n",
    "                        dupe_dict[key_header].append(uk)\n",
    "                        dupe_dict['dupe_col'].append(col)\n",
    "                        dupe_dict['dupe_count'].append(col_rows)\n",
    "                        dupe_dict['dupe_vals'].append(diff_vals)\n",
    "            \n",
    "        self.dupe_data = pl.DataFrame(dupe_dict)\n",
    "        print('Duplicates discovered')\n",
    "        return self.dupe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3055c26e-e497-4d6d-9657-be8ff9af7846",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "## Import two data sets for testing\n",
    "- superstore_with_5_dupes.csv includes 5 duplicated values\n",
    "- superstore.csv is 10,000 rows of classic Superstore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289b97ad-76da-4076-ac76-e2e577137c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dfd stands for duplicated dataframe, there are 5 added to the data\n",
    "## Import data and format column names to be lower and use _ instead of space\n",
    "dfd = pl.read_csv('superstore_with_5_dupes.csv')\n",
    "\n",
    "new_cols = {}\n",
    "for c in dfd.columns:\n",
    "    new_cols[c] = c.lower().replace(' ', '_')\n",
    "\n",
    "dfd = dfd.rename(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a777a671-f484-4aa8-bac9-97dbb4615cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df is the original or non-duplicated superstore data\n",
    "## Import data and format column names to be lower and use _ instead of space\n",
    "df = pl.read_csv('superstore.csv')\n",
    "\n",
    "new_cols = {}\n",
    "for c in df.columns:\n",
    "    new_cols[c] = c.lower().replace(' ', '_')\n",
    "\n",
    "df = df.rename(new_cols)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90455ada-9195-464e-bcfe-825c625745b1",
   "metadata": {},
   "source": [
    "## Create Dupe object\n",
    "- To start, we'll use the duplicated superstore data (dfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f05fc5b-f39f-4783-9cba-5374f99fb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create class object\n",
    "dup = Dupe(dfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932225e3-cdc3-4034-943d-36e77b68d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No key is currently assigned\n",
      "Your data includes 9999 rows and 21 columns\n"
     ]
    }
   ],
   "source": [
    "## Get info\n",
    "dup.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efee6708-c9d3-4f24-bccf-f8be734564a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub-category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>&quot;CA-2016-152156…</td><td>&quot;2016-11-08&quot;</td><td>&quot;2016-11-11&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;CG-12520&quot;</td><td>&quot;Claire Gute&quot;</td><td>&quot;Consumer&quot;</td><td>&quot;United States&quot;</td><td>&quot;Henderson&quot;</td><td>&quot;Kentucky&quot;</td><td>42420</td><td>&quot;South&quot;</td><td>&quot;FUR-BO-1000179…</td><td>&quot;Furniture&quot;</td><td>&quot;Bookcases&quot;</td><td>&quot;Bush Somerset …</td><td>261.96</td><td>2</td><td>0.0</td><td>41.9136</td></tr><tr><td>2</td><td>&quot;CA-2016-152156…</td><td>&quot;2016-11-08&quot;</td><td>&quot;2016-11-11&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;CG-12520&quot;</td><td>&quot;Claire Gute&quot;</td><td>&quot;Consumer&quot;</td><td>&quot;United States&quot;</td><td>&quot;Henderson&quot;</td><td>&quot;Kentucky&quot;</td><td>42420</td><td>&quot;South&quot;</td><td>&quot;FUR-CH-1000045…</td><td>&quot;Furniture&quot;</td><td>&quot;Chairs&quot;</td><td>&quot;Hon Deluxe Fab…</td><td>731.94</td><td>3</td><td>0.0</td><td>219.582</td></tr><tr><td>3</td><td>&quot;CA-2016-138688…</td><td>&quot;2016-06-12&quot;</td><td>&quot;2016-06-16&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;DV-13045&quot;</td><td>&quot;Darrin Van Huf…</td><td>&quot;Corporate&quot;</td><td>&quot;United States&quot;</td><td>&quot;Los Angeles&quot;</td><td>&quot;California&quot;</td><td>90036</td><td>&quot;West&quot;</td><td>&quot;OFF-LA-1000024…</td><td>&quot;Office Supplie…</td><td>&quot;Labels&quot;</td><td>&quot;Self-Adhesive …</td><td>14.62</td><td>2</td><td>0.0</td><td>6.8714</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 21)\n",
       "┌────────┬────────────────┬────────────┬────────────┬───┬────────┬──────────┬──────────┬─────────┐\n",
       "│ row_id ┆ order_id       ┆ order_date ┆ ship_date  ┆ … ┆ sales  ┆ quantity ┆ discount ┆ profit  │\n",
       "│ ---    ┆ ---            ┆ ---        ┆ ---        ┆   ┆ ---    ┆ ---      ┆ ---      ┆ ---     │\n",
       "│ i64    ┆ str            ┆ str        ┆ str        ┆   ┆ f64    ┆ i64      ┆ f64      ┆ f64     │\n",
       "╞════════╪════════════════╪════════════╪════════════╪═══╪════════╪══════════╪══════════╪═════════╡\n",
       "│ 1      ┆ CA-2016-152156 ┆ 2016-11-08 ┆ 2016-11-11 ┆ … ┆ 261.96 ┆ 2        ┆ 0.0      ┆ 41.9136 │\n",
       "│ 2      ┆ CA-2016-152156 ┆ 2016-11-08 ┆ 2016-11-11 ┆ … ┆ 731.94 ┆ 3        ┆ 0.0      ┆ 219.582 │\n",
       "│ 3      ┆ CA-2016-138688 ┆ 2016-06-12 ┆ 2016-06-16 ┆ … ┆ 14.62  ┆ 2        ┆ 0.0      ┆ 6.8714  │\n",
       "└────────┴────────────────┴────────────┴────────────┴───┴────────┴──────────┴──────────┴─────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Review the first three rows\n",
    "dup.show_data(rows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f3c5681-eed3-4dc1-b825-345f2f1ae9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 suggestions for a unique id include:\n",
      "\t1. row_id: 99.95% unique (9994 of 9999 unique)\n",
      "\t2. profit: 73.15% unique (7314 of 9999 unique)\n",
      "\t3. sales: 58.27% unique (5826 of 9999 unique)\n",
      "\t4. order_id: 50.10% unique (5009 of 9999 unique)\n",
      "\t5. product_id: 18.62% unique (1862 of 9999 unique)\n"
     ]
    }
   ],
   "source": [
    "## Get unique key value suggestions (5)\n",
    "dup.get_key_suggestion(n_suggestions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0b0186-ae31-464d-b9f0-001e0496d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks like the row_id column is the most unique\n",
    "## Let's set it as the key and find where and why it is being duplicated\n",
    "dup.set_key('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a538d003-1799-41fa-98c5-0627e1b37b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting everything up\n",
      "Starting to process 9994 rows of data\n",
      "Duplicates discovered\n"
     ]
    }
   ],
   "source": [
    "## When we run find_dupe_cols() we go through each unique value in the key column\n",
    "## Per each unique value, we iterate through each column, remove duplicates and count the total rows remaining\n",
    "## Any filtered dataframe with more than one row indicates a duplicate and is captured\n",
    "dupes = dup.find_dupe_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b0c10c-ba18-4be4-876b-8103934d866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id_value</th><th>dupe_col</th><th>dupe_count</th><th>dupe_vals</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>9990</td><td>&quot;segment&quot;</td><td>2</td><td>[&quot;Consumer&quot;, &quot;Different Value Causes Dupe&quot;]</td></tr><tr><td>9991</td><td>&quot;country&quot;</td><td>2</td><td>[&quot;United States&quot;, &quot;Different Value Causes Dupe&quot;]</td></tr><tr><td>9992</td><td>&quot;city&quot;</td><td>2</td><td>[&quot;Costa Mesa&quot;, &quot;Different Value Causes Dupe&quot;]</td></tr><tr><td>9993</td><td>&quot;postal_code&quot;</td><td>2</td><td>[&quot;999999999&quot;, &quot;92627&quot;]</td></tr><tr><td>9994</td><td>&quot;sales&quot;</td><td>2</td><td>[&quot;243.16&quot;, &quot;999999999.0&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────────┬─────────────┬────────────┬───────────────────────────────────┐\n",
       "│ row_id_value ┆ dupe_col    ┆ dupe_count ┆ dupe_vals                         │\n",
       "│ ---          ┆ ---         ┆ ---        ┆ ---                               │\n",
       "│ i64          ┆ str         ┆ i64        ┆ list[str]                         │\n",
       "╞══════════════╪═════════════╪════════════╪═══════════════════════════════════╡\n",
       "│ 9990         ┆ segment     ┆ 2          ┆ [\"Consumer\", \"Different Value Ca… │\n",
       "│ 9991         ┆ country     ┆ 2          ┆ [\"United States\", \"Different Val… │\n",
       "│ 9992         ┆ city        ┆ 2          ┆ [\"Costa Mesa\", \"Different Value … │\n",
       "│ 9993         ┆ postal_code ┆ 2          ┆ [\"999999999\", \"92627\"]            │\n",
       "│ 9994         ┆ sales       ┆ 2          ┆ [\"243.16\", \"999999999.0\"]         │\n",
       "└──────────────┴─────────────┴────────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's review the dupe_data to see which values of our key column are being duplicated and what column is responsible\n",
    "dup.dupe_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
