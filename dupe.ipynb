{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16f4327-eccc-454a-8cdc-ef281bad9239",
   "metadata": {},
   "source": [
    "# dupe Package\n",
    "The dupe package helps you to identify duplicated values in a data set that should be unique.  This package was created to assist data visualization, science, and engineering hobbiests and professionals.  Improper joins, poorly created data sets, or queries can cause problems with data in terms of level of detail leading to inaccuracies.  Analysis of duplicated values takes time and this package aims to return time back to more important tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28992a6e-02b2-4f5e-be55-05736f1119bb",
   "metadata": {},
   "source": [
    "## Import required packages\n",
    "This package relies on polars instead of pandas for the lazy option and the better speed.  There's no way to know how much data will be thrown at this package so optimizing for speed in the abstract is important for performance once in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e45a0e1-2d0c-4756-887f-a35f2921f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e08c8a8-2c6c-4a91-9cf0-0d9f1bebf924",
   "metadata": {},
   "source": [
    "## Import two data sets for testing\n",
    "- superstore_with_5_dupes.csv includes 5 duplicated values\n",
    "- superstore.csv is 10,000 rows of classic Superstore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6d08d1-258b-4e5b-aacc-1ec84c532c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data and format column names to be lower and use _ instead of space\n",
    "dfd = pl.read_csv('superstore_with_5_dupes.csv')\n",
    "\n",
    "new_cols = {}\n",
    "for c in dfd.columns:\n",
    "    new_cols[c] = c.lower().replace(' ', '_')\n",
    "\n",
    "dfd = dfd.rename(new_cols)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8115d36-3231-4acd-b3b7-38f8e1817642",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data and format column names to be lower and use _ instead of space\n",
    "df = pl.read_csv('superstore.csv')\n",
    "\n",
    "new_cols = {}\n",
    "for c in df.columns:\n",
    "    new_cols[c] = c.lower().replace(' ', '_')\n",
    "\n",
    "df = df.rename(new_cols)\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69dc850-fe2b-4b59-9cfd-0127e917cb2b",
   "metadata": {},
   "source": [
    "## Create functions for main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb818c1-64d5-48e6-94bd-ec22579558af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create class object\n",
    "class Dupe:\n",
    "    def __init__(self, data, key = None, suggest_key = False):\n",
    "        self.data = data\n",
    "        self.key = key\n",
    "        self.suggest_key = suggest_key\n",
    "        self.dupe_data = None\n",
    "\n",
    "    def set_key(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    ## Function to get or set key\n",
    "    def get_info(self):\n",
    "        message = ''\n",
    "        if self.key == None:\n",
    "            message += 'No key is currently assigned\\n'\n",
    "        else:\n",
    "            message += f'Key value: {self.key}\\n'\n",
    "\n",
    "        s = self.data.shape\n",
    "        message += f'Your data includes {s[0]} rows and {s[1]} columns'\n",
    "        print(message)\n",
    "\n",
    "    def show_data(self, rows = 5):\n",
    "        return self.data.head(rows)\n",
    "\n",
    "    def get_key_suggestion(self, n_suggestions = 3):\n",
    "        ## Get unique values for each column        \n",
    "        cols = {'column': [], 'unique': []}\n",
    "        uni = 0\n",
    "        \n",
    "        for c in self.data.columns:\n",
    "            cols['column'].append(c)\n",
    "            cols['unique'].append(self.data[c].unique().shape[0])\n",
    "        \n",
    "        ## Check if any columns are completely unique\n",
    "        suggested_keys = []\n",
    "        df_rows = self.data.shape[0]\n",
    "        sug_key = pl.DataFrame(cols)\n",
    "        \n",
    "        ## Return a list of completely unique columns\n",
    "        if sug_key.filter(pl.col('unique') > df_rows + 1).shape[0]:\n",
    "            suggested_keys = sug_key.filter(pl.col('unique') > df_rows + 1)['column'].to_list()\n",
    "        \n",
    "        ## Otherwise, return the top N options based on highest unique values\n",
    "        else:\n",
    "            suggested_keys = sug_key.sort('unique', descending = True).head(n_suggestions)['column'].to_list()\n",
    "        \n",
    "        print(f'Your data includes {df_rows} rows\\nThe top {n_suggestions} suggestions for a unique id include:')\n",
    "        \n",
    "        ([print(f\"   - {c} includes {sug_key.filter(pl.col('column') == c).select('unique').item()} unique values\") \n",
    "          for c in suggested_keys])\n",
    "\n",
    "    def find_dupe_cols(self, ignore_cols = []):\n",
    "        print('Setting everything up')\n",
    "        \n",
    "        key = self.key\n",
    "        unique_keys = self.data[key].unique().to_list()\n",
    "        uk_len = len(unique_keys)\n",
    "        uk_rows, col_rows = 0, 0\n",
    "        dupe_dict = {f'{key}_value': [], 'dupe_col': [], 'different_vals': []}\n",
    "\n",
    "        print(f'Starting to process {uk_len} rows of data')\n",
    "        count = 1\n",
    "        \n",
    "        for uk in unique_keys:\n",
    "            uk_rows = (self.data.with_columns(pl.col(key).cast(pl.String), pl.lit(1).alias('cnt'))\n",
    "             .select([key, 'cnt'])\n",
    "             .filter(pl.col(key).cast(pl.String) == str(uk))\n",
    "             .group_by(key).sum()['cnt'].item()\n",
    "            )\n",
    "        \n",
    "            if uk_rows > 1:\n",
    "                for c in [c for c in self.data.columns if c != key]:\n",
    "                    col_rows = self.data.filter(pl.col(key) == uk).select([key, c]).unique().shape[0]\n",
    "                    if col_rows > 1:\n",
    "                        dupe_dict[f'{key}_value'].append(uk)\n",
    "                        dupe_dict['dupe_col'].append(c)\n",
    "                        dupe_dict['different_vals'].append(col_rows)\n",
    "\n",
    "            print(f'Finished {count} of {uk_len} rows...', end = '\\r')\n",
    "            count += 1\n",
    "            \n",
    "        self.dupe_data = pl.DataFrame(dupe_dict)\n",
    "        return self.dupe_data\n",
    "\n",
    "    # def dupe_data(self):\n",
    "    #     if self.dupe_data != None:\n",
    "    #         return self.dupe.data\n",
    "    #     else:\n",
    "    #         print('Sorry, this Dupe does not yet inlcude dupe date.  Run find_dupe_cols first.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f05fc5b-f39f-4783-9cba-5374f99fb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = Dupe(dfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "932225e3-cdc3-4034-943d-36e77b68d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No key is currently assigned\n",
      "Your data includes 9999 rows and 21 columns\n"
     ]
    }
   ],
   "source": [
    "dup.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efee6708-c9d3-4f24-bccf-f8be734564a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub-category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>&quot;CA-2016-152156…</td><td>&quot;2016-11-08&quot;</td><td>&quot;2016-11-11&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;CG-12520&quot;</td><td>&quot;Claire Gute&quot;</td><td>&quot;Consumer&quot;</td><td>&quot;United States&quot;</td><td>&quot;Henderson&quot;</td><td>&quot;Kentucky&quot;</td><td>42420</td><td>&quot;South&quot;</td><td>&quot;FUR-BO-1000179…</td><td>&quot;Furniture&quot;</td><td>&quot;Bookcases&quot;</td><td>&quot;Bush Somerset …</td><td>261.96</td><td>2</td><td>0.0</td><td>41.9136</td></tr><tr><td>2</td><td>&quot;CA-2016-152156…</td><td>&quot;2016-11-08&quot;</td><td>&quot;2016-11-11&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;CG-12520&quot;</td><td>&quot;Claire Gute&quot;</td><td>&quot;Consumer&quot;</td><td>&quot;United States&quot;</td><td>&quot;Henderson&quot;</td><td>&quot;Kentucky&quot;</td><td>42420</td><td>&quot;South&quot;</td><td>&quot;FUR-CH-1000045…</td><td>&quot;Furniture&quot;</td><td>&quot;Chairs&quot;</td><td>&quot;Hon Deluxe Fab…</td><td>731.94</td><td>3</td><td>0.0</td><td>219.582</td></tr><tr><td>3</td><td>&quot;CA-2016-138688…</td><td>&quot;2016-06-12&quot;</td><td>&quot;2016-06-16&quot;</td><td>&quot;Second Class&quot;</td><td>&quot;DV-13045&quot;</td><td>&quot;Darrin Van Huf…</td><td>&quot;Corporate&quot;</td><td>&quot;United States&quot;</td><td>&quot;Los Angeles&quot;</td><td>&quot;California&quot;</td><td>90036</td><td>&quot;West&quot;</td><td>&quot;OFF-LA-1000024…</td><td>&quot;Office Supplie…</td><td>&quot;Labels&quot;</td><td>&quot;Self-Adhesive …</td><td>14.62</td><td>2</td><td>0.0</td><td>6.8714</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 21)\n",
       "┌────────┬────────────────┬────────────┬────────────┬───┬────────┬──────────┬──────────┬─────────┐\n",
       "│ row_id ┆ order_id       ┆ order_date ┆ ship_date  ┆ … ┆ sales  ┆ quantity ┆ discount ┆ profit  │\n",
       "│ ---    ┆ ---            ┆ ---        ┆ ---        ┆   ┆ ---    ┆ ---      ┆ ---      ┆ ---     │\n",
       "│ i64    ┆ str            ┆ str        ┆ str        ┆   ┆ f64    ┆ i64      ┆ f64      ┆ f64     │\n",
       "╞════════╪════════════════╪════════════╪════════════╪═══╪════════╪══════════╪══════════╪═════════╡\n",
       "│ 1      ┆ CA-2016-152156 ┆ 2016-11-08 ┆ 2016-11-11 ┆ … ┆ 261.96 ┆ 2        ┆ 0.0      ┆ 41.9136 │\n",
       "│ 2      ┆ CA-2016-152156 ┆ 2016-11-08 ┆ 2016-11-11 ┆ … ┆ 731.94 ┆ 3        ┆ 0.0      ┆ 219.582 │\n",
       "│ 3      ┆ CA-2016-138688 ┆ 2016-06-12 ┆ 2016-06-16 ┆ … ┆ 14.62  ┆ 2        ┆ 0.0      ┆ 6.8714  │\n",
       "└────────┴────────────────┴────────────┴────────────┴───┴────────┴──────────┴──────────┴─────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup.show_data(rows = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f3c5681-eed3-4dc1-b825-345f2f1ae9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data includes 9999 rows\n",
      "The top 3 suggestions for a unique id include:\n",
      "   - row_id includes 9994 unique values\n",
      "   - profit includes 7314 unique values\n",
      "   - sales includes 5826 unique values\n"
     ]
    }
   ],
   "source": [
    "dup.get_key_suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a0b0186-ae31-464d-b9f0-001e0496d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup.set_key('row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a538d003-1799-41fa-98c5-0627e1b37b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting everything up\n",
      "Starting to process 9994 rows of data\n",
      "Finished 9994 of 9994 rows...\r"
     ]
    }
   ],
   "source": [
    "dupes = dup.find_dupe_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58b0c10c-ba18-4be4-876b-8103934d866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id_value</th><th>dupe_col</th><th>different_vals</th></tr><tr><td>i64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>9990</td><td>&quot;segment&quot;</td><td>2</td></tr><tr><td>9991</td><td>&quot;country&quot;</td><td>2</td></tr><tr><td>9992</td><td>&quot;city&quot;</td><td>2</td></tr><tr><td>9993</td><td>&quot;postal_code&quot;</td><td>2</td></tr><tr><td>9994</td><td>&quot;sales&quot;</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌──────────────┬─────────────┬────────────────┐\n",
       "│ row_id_value ┆ dupe_col    ┆ different_vals │\n",
       "│ ---          ┆ ---         ┆ ---            │\n",
       "│ i64          ┆ str         ┆ i64            │\n",
       "╞══════════════╪═════════════╪════════════════╡\n",
       "│ 9990         ┆ segment     ┆ 2              │\n",
       "│ 9991         ┆ country     ┆ 2              │\n",
       "│ 9992         ┆ city        ┆ 2              │\n",
       "│ 9993         ┆ postal_code ┆ 2              │\n",
       "│ 9994         ┆ sales       ┆ 2              │\n",
       "└──────────────┴─────────────┴────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup.dupe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab215958-19b4-4fcd-9da7-b0c1e825ce73",
   "metadata": {},
   "source": [
    "## Package Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb16ebd-c827-4c07-a6fc-a1d7a8967ae9",
   "metadata": {},
   "source": [
    "## Code Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f47ba-b707-4889-b3ae-363eb36292bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The 'key' variable is the column for which there should be unique values.\n",
    "## The entire point of this code is to identify what other columns cause the duplication\n",
    "key = 'row_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de6890-dbc4-4a3b-a845-2ab6e5a84086",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we test to see if the value is unique\n",
    "total_values = df[key].shape[0]\n",
    "unique_values = df[key].unique().shape[0]\n",
    "\n",
    "total_values == unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689f8e4-b303-4fe1-ba43-c20f4c13d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93039f-58dc-4af0-b957-2d693b707f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'row_id'\n",
    "\n",
    "unique_keys = df[key].unique().to_list()\n",
    "uk_rows, col_rows = 0, 0\n",
    "dupe_dict = {'value': [], 'val_dupes': [], 'dupe_col': []}\n",
    "\n",
    "for uk in unique_keys[-8:]:\n",
    "    uk_rows = (dfd.with_columns('row_id', pl.lit(1).alias('cnt'))\n",
    "     .select(['row_id', 'cnt'])\n",
    "     .filter(pl.col('row_id') == uk)\n",
    "     .group_by('row_id').sum()['cnt'].item()\n",
    "    )\n",
    "\n",
    "    if uk_rows > 1:\n",
    "        for c in [c for c in dfd.columns if c != key]:\n",
    "            col_rows = dfd.filter(pl.col(key) == uk).select([key, c]).unique().shape[0]\n",
    "            if col_rows > 1:\n",
    "                dupe_dict['value'].append(uk)\n",
    "                dupe_dict['val_dupes'].append(col_rows)\n",
    "                dupe_dict['dupe_col'].append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d1415-533e-42e0-b50a-3aee80388e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = pl.DataFrame(dupe_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c4e52-a70b-416f-9ed4-630772d8bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156bf1d4-0688-4cde-b01a-90b40505dba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f170650-12bc-4fe3-9317-a22a6d99074d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f4162-7c40-47a0-ab69-2e4156574a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658cd016-972b-439c-8b5c-cc23fea553bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {'run': [], 'time': []}\n",
    "job_start = time.time()\n",
    "\n",
    "## We will have to go over the dataframe by the key value and then investigate the uniqueness of every column\n",
    "## Where we have multiple values, we have identified columns that cause duplication and those that do not\n",
    "unique_keys = df[key].unique().to_list()\n",
    "rows = 0\n",
    "dupe_cols = []\n",
    "has_dupes = False\n",
    "cur_df = pl.DataFrame()\n",
    "cols = [c for c in df.columns if c != key]\n",
    "results = {'key': [], 'rows': [], 'dupe_cols': []}\n",
    "\n",
    "for uk in unique_keys:\n",
    "    loop_start = time.time()\n",
    "    run = 1\n",
    "    ## Reset variables\n",
    "    has_dupes = False\n",
    "    dup_cols = ['']\n",
    "\n",
    "    ## Create partition and capture rows\n",
    "    cur_df = df.filter(pl.col(key) == uk)\n",
    "    rows = cur_df.shape[0]\n",
    "\n",
    "    # ## Test for duplicates for that key value\n",
    "    # if rows > 1:\n",
    "    #     for c in cols:\n",
    "    #         if cur_df.select([key, c]).unique().shape[0] > 0:\n",
    "    #             dupe_cols.append(c)\n",
    "\n",
    "\n",
    "    results['key'].append(uk)\n",
    "    results['rows'].append(rows)\n",
    "    results['dupe_cols'].append(dupe_cols)\n",
    "    \n",
    "    end = time.time()\n",
    "    time_dict['run'].append(run)\n",
    "    time_dict['time'].append(end - loop_start)\n",
    "    print(f'Finished {uk} with a run time of {end - loop_start}', end = '\\r')\n",
    "    run += 1\n",
    "\n",
    "end = time.time()\n",
    "time_dict['run'].append('total')\n",
    "time_dict['time'].append(end - start)\n",
    "\n",
    "print(f'Total job finished with runtime {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7326fa-b084-4bac-8fcc-338b84ad5c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020f479-54f3-492f-9386-88a424579129",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {'run': [], 'time': []}\n",
    "job_start = time.time()\n",
    "\n",
    "## We will have to go over the dataframe by the key value and then investigate the uniqueness of every column\n",
    "## Where we have multiple values, we have identified columns that cause duplication and those that do not\n",
    "unique_keys = df[key].unique().to_list()\n",
    "rows = 0\n",
    "dupe_cols = []\n",
    "has_dupes = False\n",
    "cur_df = pl.DataFrame()\n",
    "cols = [c for c in df.columns if c != key]\n",
    "results = {'key': [], 'rows': [], 'dupe_cols': []}\n",
    "\n",
    "for uk in unique_keys:\n",
    "    loop_start = time.time()\n",
    "    run = 1\n",
    "    ## Reset variables\n",
    "    has_dupes = False\n",
    "    dup_cols = ['']\n",
    "\n",
    "    ## Create partition and capture rows\n",
    "    cur_df = df.filter(pl.col(key) == uk)\n",
    "    rows = cur_df.shape[0]\n",
    "\n",
    "    ## Test for duplicates for that key value\n",
    "    if rows > 1:\n",
    "        for c in cols:\n",
    "            if cur_df.select([key, c]).unique().shape[0] > 0:\n",
    "                dupe_cols.append(c)\n",
    "\n",
    "\n",
    "    results['key'].append(uk)\n",
    "    results['rows'].append(rows)\n",
    "    results['dupe_cols'].append(dupe_cols)\n",
    "    \n",
    "    end = time.time()\n",
    "    time_dict['run'].append(run)\n",
    "    time_dict['time'].append(end - loop_start)\n",
    "    print(f'Finished {uk} with a run time of {end - loop_start}', end = '\\r')\n",
    "    run += 1\n",
    "\n",
    "end = time.time()\n",
    "time_dict['run'].append('total')\n",
    "time_dict['time'].append(end - start)\n",
    "\n",
    "print(f'Total job finished with runtime {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be73f7-2df3-4769-bc27-50b725339acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(end - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1171e-3ec9-46ba-8f6a-79666bcb0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame(results).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54a6a7-a89c-4182-bd08-c9678ecacdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdf = pl.DataFrame(results)\n",
    "# rdf = rdf.explode('dupe_cols')\n",
    "# rdf = rdf.filter(pl.col('rows') > 1).sort(['rows', 'key'])\n",
    "# rdf.filter(pl.col('key') == 9990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938b918-bdde-4479-960f-e10a0afd9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dupe_list = rdf.filter(pl.col('rows') > 1)['dupe_cols'].unique().to_list()\n",
    "# dupe_list.sort()\n",
    "# dupe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39671b4-e0a5-449c-82d3-d80357e41e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1407e2-0523-4e0e-8108-0e337bb87eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    df = pl.concat([df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6bb7d7-7773-4e7c-8206-413e5cbf07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "uni = df['order_id'].unique().shape[0]\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf68193-3fb6-4698-b989-8f0ae9cc813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c69e2-fa98-4f9d-9517-735087826210",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "dup_keys = (df.with_columns(pl.lit(1).alias('count'))\n",
    "            .select(['order_id', 'count'])\n",
    "            .group_by('order_id').sum()\n",
    "            .filter(pl.col('count') > 1)\n",
    "           )\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccb637-7b70-4b8c-a675-fa0a1ca2d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095d420-2c18-46af-bab1-1c3abb87a08e",
   "metadata": {},
   "source": [
    "## suggest key code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b67ad-efe1-4237-bfcf-85ef9e328bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e92cdf-5dc3-49ab-9ef7-48c29608d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get unique values for each column\n",
    "# n_suggestions = 3\n",
    "\n",
    "# cols = {'column': [], 'unique': []}\n",
    "# uni = 0\n",
    "\n",
    "# for c in df.columns:\n",
    "#     cols['column'].append(c)\n",
    "#     cols['unique'].append(df[c].unique().shape[0])\n",
    "\n",
    "# ## Check if any columns are completely unique\n",
    "# suggested_keys = []\n",
    "# df_rows = df.shape[0]\n",
    "# sug_key = pl.DataFrame(cols)\n",
    "\n",
    "# if sug_key.filter(pl.col('unique') > df_rows + 1).shape[0]:\n",
    "#     suggested_keys = sug_key.filter(pl.col('unique') > df_rows + 1)['column'].to_list()\n",
    "\n",
    "# else:\n",
    "#     suggested_keys = sug_key.sort('unique', descending = True).head(n_suggestions)['column'].to_list()\n",
    "\n",
    "# suggested_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79900e9-36bd-49ab-9ba2-0241ec1bd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get unique values for each column        \n",
    "# cols = {'column': [], 'unique': []}\n",
    "# uni = 0\n",
    "\n",
    "# for c in df.columns:\n",
    "#     cols['column'].append(c)\n",
    "#     cols['unique'].append(df[c].unique().shape[0])\n",
    "\n",
    "# ## Check if any columns are completely unique\n",
    "# suggested_keys = []\n",
    "# df_rows = df.shape[0]\n",
    "# sug_key = pl.DataFrame(cols)\n",
    "\n",
    "# ## Return a list of completely unique columns\n",
    "# if sug_key.filter(pl.col('unique') > df_rows + 1).shape[0]:\n",
    "#     suggested_keys = sug_key.filter(pl.col('unique') > df_rows + 1)['column'].to_list()\n",
    "\n",
    "# ## Otherwise, return the top N options based on highest unique values\n",
    "# else:\n",
    "#     suggested_keys = sug_key.sort('unique', descending = True).head(n_suggestions)['column'].to_list()\n",
    "\n",
    "# print(f'Your data includes {df_rows} rows\\nThe top {n_suggestions} suggestions for a unique id include:')\n",
    "\n",
    "# ([print(f\"   - {c} includes {sug_key.filter(pl.col('column') == 'row_id').select('unique').item()} unique values\") \n",
    "#   for c in suggested_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0a161-8069-4d6f-9ce9-3ce4c9648c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_suggestions = 3\n",
    "\n",
    "# ## Get unique values for each column        \n",
    "# cols = {'column': [], 'unique': []}\n",
    "# uni = 0\n",
    "\n",
    "# for c in df.columns:\n",
    "#     cols['column'].append(c)\n",
    "#     cols['unique'].append(df[c].unique().shape[0])\n",
    "\n",
    "# ## Check if any columns are completely unique\n",
    "# suggested_keys = []\n",
    "# df_rows = df.shape[0]\n",
    "# sug_key = pl.DataFrame(cols)\n",
    "\n",
    "# ## Return a list of completely unique columns\n",
    "# if sug_key.filter(pl.col('unique') > df_rows + 1).shape[0]:\n",
    "#     suggested_keys = sug_key.filter(pl.col('unique') > df_rows + 1)['column'].to_list()\n",
    "\n",
    "# ## Otherwise, return the top N options based on highest unique values\n",
    "# else:\n",
    "#     suggested_keys = sug_key.sort('unique', descending = True).head(n_suggestions)['column'].to_list()\n",
    "\n",
    "# print(f'Your data includes {df_rows} rows\\nThe top {n_suggestions} suggestions for a unique id include:')\n",
    "\n",
    "# ([print(f\"   - {c} includes {sug_key.filter(pl.col('column') == 'row_id').select('unique').item()} unique values\") \n",
    "#   for c in suggested_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638cf118-3d96-4665-9623-33a9a418f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_key.filter(pl.col('column') == 'row_id').select('unique').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c73ff-41a8-4c36-9dd5-b4b640dfd10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
